{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "binary_path: c:\\Python311\\Lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "CUDA SETUP: Loading binary c:\\Python311\\Lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2ForSequenceClassification, GPT2Tokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
    "\n",
    "# IMPORTANT: Update the padding token ID in the model configuration\n",
    "model.config.pad_token_id = model.config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "# This file was generate with GPT-4. I asked it to randomly generate answers to a set of questions, and then pick a preferred answer for the annotation. \n",
    "# This would have to be replaced by the desired dataset and the 'answer1' and 'answer2' annotations may be changed. If changed, I need to update the following cell where the labels are extracted.\n",
    "data_path = './dataset_reward_model.json'\n",
    "\n",
    "with codecs.open(data_path, 'r', encoding='utf-8') as f:\n",
    "      dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "prompts = [item['data']['prompt'] for item in dataset]\n",
    "answer1s = [item['data']['answer1'] for item in dataset]\n",
    "answer2s = [item['data']['answer2'] for item in dataset]\n",
    "labels_list = [0 if item['data']['annotation'] == 'answer1' else 1 for item in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "max_length = 100  # Choose a max_length that fits your data\n",
    "encodings = tokenizer(prompts, answer1s, answer2s, truncation=True, padding='max_length', max_length=max_length)\n",
    "input_ids = torch.tensor(encodings['input_ids'])\n",
    "attention_mask = torch.tensor(encodings['attention_mask'])\n",
    "labels = torch.tensor(labels_list)\n",
    "labels = labels.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)  # Use an appropriate batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Batch 1/16, Loss: 0.3626187741756439\n",
      "Epoch 1/3, Batch 2/16, Loss: 0.0004944835091009736\n",
      "Epoch 1/3, Batch 3/16, Loss: 8.265109062194824\n",
      "Epoch 1/3, Batch 4/16, Loss: 0.0030704401433467865\n",
      "Epoch 1/3, Batch 5/16, Loss: 4.266773223876953\n",
      "Epoch 1/3, Batch 6/16, Loss: 0.0013885501539334655\n",
      "Epoch 1/3, Batch 7/16, Loss: 0.003802233375608921\n",
      "Epoch 1/3, Batch 8/16, Loss: 0.25442084670066833\n",
      "Epoch 1/3, Batch 9/16, Loss: 3.107051134109497\n",
      "Epoch 1/3, Batch 10/16, Loss: 12.563957214355469\n",
      "Epoch 1/3, Batch 11/16, Loss: 6.96302604675293\n",
      "Epoch 1/3, Batch 12/16, Loss: 3.980490207672119\n",
      "Epoch 1/3, Batch 13/16, Loss: 8.177164077758789\n",
      "Epoch 1/3, Batch 14/16, Loss: 3.5492444038391113\n",
      "Epoch 1/3, Batch 15/16, Loss: 5.57310676574707\n",
      "Epoch 1/3, Batch 16/16, Loss: 3.62490177154541\n",
      "Epoch 2/3, Batch 1/16, Loss: 0.354430228471756\n",
      "Epoch 2/3, Batch 2/16, Loss: 2.85650897026062\n",
      "Epoch 2/3, Batch 3/16, Loss: 2.8355190753936768\n",
      "Epoch 2/3, Batch 4/16, Loss: 0.029429355636239052\n",
      "Epoch 2/3, Batch 5/16, Loss: 2.9311890602111816\n",
      "Epoch 2/3, Batch 6/16, Loss: 3.7441563606262207\n",
      "Epoch 2/3, Batch 7/16, Loss: 3.3511812686920166\n",
      "Epoch 2/3, Batch 8/16, Loss: 3.528104305267334\n",
      "Epoch 2/3, Batch 9/16, Loss: 2.545949697494507\n",
      "Epoch 2/3, Batch 10/16, Loss: 2.9103081226348877\n",
      "Epoch 2/3, Batch 11/16, Loss: 2.956402540206909\n",
      "Epoch 2/3, Batch 12/16, Loss: 0.3852387070655823\n",
      "Epoch 2/3, Batch 13/16, Loss: 0.6664459109306335\n",
      "Epoch 2/3, Batch 14/16, Loss: 0.054308757185935974\n",
      "Epoch 2/3, Batch 15/16, Loss: 2.473574161529541\n",
      "Epoch 2/3, Batch 16/16, Loss: 1.8322800397872925\n",
      "Epoch 3/3, Batch 1/16, Loss: 2.7427330017089844\n",
      "Epoch 3/3, Batch 2/16, Loss: 0.04411690682172775\n",
      "Epoch 3/3, Batch 3/16, Loss: 4.012050628662109\n",
      "Epoch 3/3, Batch 4/16, Loss: 0.017403503879904747\n",
      "Epoch 3/3, Batch 5/16, Loss: 0.0051837339997291565\n",
      "Epoch 3/3, Batch 6/16, Loss: 1.4622598886489868\n",
      "Epoch 3/3, Batch 7/16, Loss: 0.005748862866312265\n",
      "Epoch 3/3, Batch 8/16, Loss: 0.8240594267845154\n",
      "Epoch 3/3, Batch 9/16, Loss: 0.0019600465893745422\n",
      "Epoch 3/3, Batch 10/16, Loss: 2.4136295318603516\n",
      "Epoch 3/3, Batch 11/16, Loss: 2.511719226837158\n",
      "Epoch 3/3, Batch 12/16, Loss: 0.8757904767990112\n",
      "Epoch 3/3, Batch 13/16, Loss: 0.03910183906555176\n",
      "Epoch 3/3, Batch 14/16, Loss: 2.21355938911438\n",
      "Epoch 3/3, Batch 15/16, Loss: 6.258185021579266e-05\n",
      "Epoch 3/3, Batch 16/16, Loss: 0.5219983458518982\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "epochs = 3\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for i, batch in enumerate(loader):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Batch {i+1}/{len(loader)}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model_rewards_model\\\\tokenizer_config.json',\n",
       " 'model_rewards_model\\\\special_tokens_map.json',\n",
       " 'model_rewards_model\\\\vocab.json',\n",
       " 'model_rewards_model\\\\merges.txt',\n",
       " 'model_rewards_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model to a directory\n",
    "save_directory = \"model_rewards_model\"\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Optionally, save the tokenizer as well, especially if you've added special tokens or made other changes\n",
    "tokenizer.save_pretrained(save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def calc_reward(model, tokenizer, prompt, answer1, answer2):\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, [answer1, answer2], return_tensors='pt', padding=True, truncation=True, max_length=100)\n",
    "\n",
    "    # Get model output\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Calculate probabilities\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # Interpret the result\n",
    "    return logits \n",
    "\n",
    "    if probs[0, 0] > probs[0, 1]:\n",
    "        print(f\"The model prefers '{answer1}' with a probability of {probs[0, 0]:.4f}\")\n",
    "    else:\n",
    "        print(f\"The model prefers '{answer2}' with a probability of {probs[0, 1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.6558, 6.6592]])\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "prompt = \"What are the latest developments in artificial intelligence?\"\n",
    "answer1 = \"GANs are revolutionizing image creation, and NLP models like GPT-3 are transforming language tasks.\"\n",
    "answer2 = \"AI is making strides in healthcare for diagnosis, and reinforcement learning is advancing robotics.\"\n",
    "\n",
    "logits = calc_reward(model, tokenizer, prompt, answer1, answer2)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
